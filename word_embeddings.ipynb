{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "111a3311-7489-4b43-b0df-afeb8f684ba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAssignment 10 - Word Embeddings\\n\\nSkip-gram approach\\nTraining on a corpus of Shakespeare and\\nA corpus of contemporary language\\n\\nTest on the classification of movie reviews. \\n\\n\\n\\nAdda Weathers\\nFebruary 23, 2025\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Assignment 10 - Word Embeddings\n",
    "\n",
    "Skip-gram approach\n",
    "Training on a corpus of Shakespeare\n",
    "\n",
    "Test on the classification of movie reviews. \n",
    "\n",
    "\n",
    "\n",
    "Adda Weathers\n",
    "February 23, 2025\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b8a8188-6db5-41f0-b631-8f8f15a6be1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries successfully installed.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Libraries\n",
    "import io\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "# Testing to make sure these libraries installed without error\n",
    "print(\"Libraries successfully installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ff4ca83-d634-4765-a469-97c56a4234e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TensorBoard ext\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d37011ad-6ea4-4e52-b549-998f27f159f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8730034f-9e57-410f-b256-e2a615b8d42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "Shakesparean Sentence here.\n"
     ]
    }
   ],
   "source": [
    "sentence = \"He hath eaten me out of house and home He ate so much there was nothing left\"\n",
    "tokens = list(sentence.lower().split())\n",
    "print(len(tokens))\n",
    "print(\"Shakesparean Sentence here.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a3ca760-1472-4e72-9612-719ebbd3553b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0, 'he': 1, 'hath': 2, 'eaten': 3, 'me': 4, 'out': 5, 'of': 6, 'house': 7, 'and': 8, 'home': 9, 'ate': 10, 'so': 11, 'much': 12, 'there': 13, 'was': 14, 'nothing': 15, 'left': 16}\n",
      "Index and padding check\n"
     ]
    }
   ],
   "source": [
    "vocab, index = {}, 1  # starting index from 1\n",
    "vocab['<pad>'] = 0  # padding token here\n",
    "for token in tokens:\n",
    "  if token not in vocab:\n",
    "    vocab[token] = index\n",
    "    index += 1\n",
    "vocab_size = len(vocab)\n",
    "print(vocab)\n",
    "print(\"Index and padding check\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71010809-1bcf-41f3-8577-f69d96c220f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<pad>', 1: 'he', 2: 'hath', 3: 'eaten', 4: 'me', 5: 'out', 6: 'of', 7: 'house', 8: 'and', 9: 'home', 10: 'ate', 11: 'so', 12: 'much', 13: 'there', 14: 'was', 15: 'nothing', 16: 'left'}\n",
      "Creating inverse vocabulary\n"
     ]
    }
   ],
   "source": [
    "inverse_vocab = {index: token for token, index in vocab.items()}\n",
    "print(inverse_vocab)\n",
    "print(\"Creating inverse vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60f357f3-7bf1-455a-95ec-9557610c742e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 10, 11, 12, 13, 14, 15, 16]\n",
      "Vectorizing the sentence\n"
     ]
    }
   ],
   "source": [
    "example_sequence = [vocab[word] for word in tokens]\n",
    "print(example_sequence)\n",
    "print(\"Vectorizing the sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed871bf6-7098-4327-bf95-3d0c182d1254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we will do the skipgrams.\n",
      "62\n"
     ]
    }
   ],
   "source": [
    "print(\"Now we will do the skipgrams.\")\n",
    "window_size = 2\n",
    "positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "      example_sequence,\n",
    "      vocabulary_size=vocab_size,\n",
    "      window_size=window_size,\n",
    "      negative_samples=0)\n",
    "print(len(positive_skip_grams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "119cbcf0-9760-4c34-b8d9-05deb842cba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we will do a few positive skipgrams.\n",
      "(5, 4): (out, me)\n",
      "(6, 8): (of, and)\n",
      "(15, 13): (nothing, there)\n",
      "(11, 10): (so, ate)\n",
      "(2, 3): (hath, eaten)\n"
     ]
    }
   ],
   "source": [
    "print(\"Now we will do a few positive skipgrams.\")\n",
    "for target, context in positive_skip_grams[:5]:\n",
    "  print(f\"({target}, {context}): ({inverse_vocab[target]}, {inverse_vocab[context]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cde66f8-dbe5-4e4a-8d62-71a2100c82d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative sampling for a skipgram.\n",
      "tf.Tensor([4 1 9 3], shape=(4,), dtype=int64)\n",
      "['me', 'he', 'home', 'eaten']\n"
     ]
    }
   ],
   "source": [
    "print(\"Negative sampling for a skipgram.\")\n",
    "# Get target and context words for one positive skip-gram.\n",
    "target_word, context_word = positive_skip_grams[0]\n",
    "\n",
    "# Set the number of negative samples per positive context.\n",
    "num_ns = 4\n",
    "\n",
    "context_class = tf.reshape(tf.constant(context_word, dtype=\"int64\"), (1, 1))\n",
    "negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "    true_classes=context_class,  # class that should be sampled as 'positive'\n",
    "    num_true=1,  # each positive skip-gram has 1 positive context class\n",
    "    num_sampled=num_ns,  # number of negative context words to sample\n",
    "    unique=True,  # all the negative samples should be unique\n",
    "    range_max=vocab_size,  # pick index of the samples from [0, vocab_size]\n",
    "    seed=SEED,  # seed for reproducibility\n",
    "    name=\"negative_sampling\"  # name of this operation\n",
    ")\n",
    "print(negative_sampling_candidates)\n",
    "print([inverse_vocab[index.numpy()] for index in negative_sampling_candidates])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6045085-438a-47bb-97c8-7ef12d973197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing one training example now.\n"
     ]
    }
   ],
   "source": [
    "print(\"Constructing one training example now.\")\n",
    "# Reduce a dimension to use concatenation (in the next step).\n",
    "squeezed_context_class = tf.squeeze(context_class, 1)\n",
    "\n",
    "# Concatenate a positive context word with negative sampled words.\n",
    "context = tf.concat([squeezed_context_class, negative_sampling_candidates], 0)\n",
    "\n",
    "# Label the first context word as `1` (positive) followed by `num_ns` `0`s (negative).\n",
    "label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "target = target_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce51b946-b1a3-4781-a170-3b60563e517f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_index    : 5\n",
      "target_word     : out\n",
      "context_indices : [4 4 1 9 3]\n",
      "context_words   : ['me', 'me', 'he', 'home', 'eaten']\n",
      "label           : [1 0 0 0 0]\n",
      "Context and corresponding labels now.\n"
     ]
    }
   ],
   "source": [
    "print(f\"target_index    : {target}\")\n",
    "print(f\"target_word     : {inverse_vocab[target_word]}\")\n",
    "print(f\"context_indices : {context}\")\n",
    "print(f\"context_words   : {[inverse_vocab[c.numpy()] for c in context]}\")\n",
    "print(f\"label           : {label}\")\n",
    "print(\"Context and corresponding labels now.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dff10111-4a29-4788-b1d9-173a43a5e35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target  : 5\n",
      "context : tf.Tensor([4 4 1 9 3], shape=(5,), dtype=int64)\n",
      "label   : tf.Tensor([1 0 0 0 0], shape=(5,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(\"target  :\", target)\n",
    "print(\"context :\", context)\n",
    "print(\"label   :\", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87eb0b88-148e-4220-8887-602f9b0a6420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compile it all into one function.\n",
      "[0.00315225 0.00315225 0.00547597 0.00741556 0.00912817 0.01068435\n",
      " 0.01212381 0.01347162 0.01474487 0.0159558 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"Compile it all into one function.\")\n",
    "sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(size=10)\n",
    "print(sampling_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8580acc-4840-4a6d-9a9e-dff12f28e338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
    "# (int-encoded sentences) based on window size, number of negative samples\n",
    "# and vocabulary size.\n",
    "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
    "  # Elements of each training example are appended to these lists.\n",
    "  targets, contexts, labels = [], [], []\n",
    "\n",
    "  # Build the sampling table for `vocab_size` tokens.\n",
    "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "  # Iterate over all sequences (sentences) in the dataset.\n",
    "  for sequence in tqdm.tqdm(sequences):\n",
    "\n",
    "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
    "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "          sequence,\n",
    "          vocabulary_size=vocab_size,\n",
    "          sampling_table=sampling_table,\n",
    "          window_size=window_size,\n",
    "          negative_samples=0)\n",
    "\n",
    "    # Iterate over each positive skip-gram pair to produce training examples\n",
    "    # with a positive context word and negative samples.\n",
    "    for target_word, context_word in positive_skip_grams:\n",
    "      context_class = tf.expand_dims(\n",
    "          tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "          true_classes=context_class,\n",
    "          num_true=1,\n",
    "          num_sampled=num_ns,\n",
    "          unique=True,\n",
    "          range_max=vocab_size,\n",
    "          seed=seed,\n",
    "          name=\"negative_sampling\")\n",
    "\n",
    "      # Build context and label vectors (for one target word)\n",
    "      context = tf.concat([tf.squeeze(context_class,1), negative_sampling_candidates], 0)\n",
    "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "      # Append each element from the training example to global lists.\n",
    "      targets.append(target_word)\n",
    "      contexts.append(context)\n",
    "      labels.append(label)\n",
    "\n",
    "  return targets, contexts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a28a0b88-fe58-457b-be57-4dbba298440d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06898279-d834-4bf4-ab5d-f6ef3c1b1efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n"
     ]
    }
   ],
   "source": [
    "with open(path_to_file) as f:\n",
    "  lines = f.read().splitlines()\n",
    "for line in lines[:20]:\n",
    "  print(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "620efcae-bdea-47a6-a4f6-56b48cbd92e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing TextLineDataset\n"
     ]
    }
   ],
   "source": [
    "text_ds = tf.data.TextLineDataset(path_to_file).filter(lambda x: tf.cast(tf.strings.length(x), bool))\n",
    "print(\"Constructing TextLineDataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc911ae3-d9e9-4ff3-bb8c-cb5c2dc97467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorize sentences from the corpus\n"
     ]
    }
   ],
   "source": [
    "print(\"Vectorize sentences from the corpus\")\n",
    "# Now, create a custom standardization function to lowercase the text and\n",
    "# remove punctuation.\n",
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  return tf.strings.regex_replace(lowercase,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "\n",
    "# Define the vocabulary size and the number of words in a sequence.\n",
    "vocab_size = 4096\n",
    "sequence_length = 10\n",
    "\n",
    "# Use the `TextVectorization` layer to normalize, split, and map strings to\n",
    "# integers. Set the `output_sequence_length` length to pad all samples to the\n",
    "# same length.\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c83fe5ee-234b-4612-a923-d74271b19d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create vocabulary\n"
     ]
    }
   ],
   "source": [
    "print(\"Create vocabulary\")\n",
    "vectorize_layer.adapt(text_ds.batch(1024))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4fd6aead-8fc6-43cb-ac02-f20af1dbf04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save created vocab for reference.\n",
      "['', '[UNK]', 'the', 'and', 'to', 'i', 'of', 'you', 'my', 'a', 'that', 'in', 'is', 'not', 'for', 'with', 'me', 'it', 'be', 'your']\n"
     ]
    }
   ],
   "source": [
    "print(\"Save created vocab for reference.\")\n",
    "# Save the created vocabulary for reference.\n",
    "inverse_vocab = vectorize_layer.get_vocabulary()\n",
    "print(inverse_vocab[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5baf0f5d-35be-4263-a278-eef98581e18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorize data\n"
     ]
    }
   ],
   "source": [
    "print(\"Vectorize data\")\n",
    "# Vectorize the data in text_ds.\n",
    "text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "19a40966-4fa2-4ab1-b2d2-ff3971a75f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtain sequences from the dataset.\n",
      "32777\n"
     ]
    }
   ],
   "source": [
    "print(\"Obtain sequences from the dataset.\")\n",
    "sequences = list(text_vector_ds.as_numpy_iterator())\n",
    "print(len(sequences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f697bb4-8ad2-4e7f-b7fc-735586612ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting a few examples from the sequence we just made.\n",
      "[ 89 270   0   0   0   0   0   0   0   0] => ['first', 'citizen', '', '', '', '', '', '', '', '']\n",
      "[138  36 982 144 673 125  16 106   0   0] => ['before', 'we', 'proceed', 'any', 'further', 'hear', 'me', 'speak', '', '']\n",
      "[34  0  0  0  0  0  0  0  0  0] => ['all', '', '', '', '', '', '', '', '', '']\n",
      "[106 106   0   0   0   0   0   0   0   0] => ['speak', 'speak', '', '', '', '', '', '', '', '']\n",
      "[ 89 270   0   0   0   0   0   0   0   0] => ['first', 'citizen', '', '', '', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "print(\"Inspecting a few examples from the sequence we just made.\")\n",
    "for seq in sequences[:5]:\n",
    "  print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8004e5b2-2776-4269-aba8-d6ff67b69f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating training examples from sequence.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 32777/32777 [01:11<00:00, 459.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "targets.shape: (65080,)\n",
      "contexts.shape: (65080, 5)\n",
      "labels.shape: (65080, 5)\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating training examples from sequence.\")\n",
    "targets, contexts, labels = generate_training_data(\n",
    "    sequences=sequences,\n",
    "    window_size=2,\n",
    "    num_ns=4,\n",
    "    vocab_size=vocab_size,\n",
    "    seed=SEED)\n",
    "\n",
    "targets = np.array(targets)\n",
    "contexts = np.array(contexts)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print('\\n')\n",
    "print(f\"targets.shape: {targets.shape}\")\n",
    "print(f\"contexts.shape: {contexts.shape}\")\n",
    "print(f\"labels.shape: {labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e0925a7a-e169-4e0c-95cf-da4c643e4073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring dataset for performance.\n",
      "<_BatchDataset element_spec=((TensorSpec(shape=(1024,), dtype=tf.int64, name=None), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None)), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "print(\"Configuring dataset for performance.\")\n",
    "BATCH_SIZE = 1024\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c2f513a6-298c-4937-92b6-f4c97399c76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying Dataset.Cache to improve performance.\n",
      "<_PrefetchDataset element_spec=((TensorSpec(shape=(1024,), dtype=tf.int64, name=None), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None)), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "print(\"Applying Dataset.Cache to improve performance.\")\n",
    "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5c0d4aff-8e50-4e5f-9851-6d265f886ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and training\n"
     ]
    }
   ],
   "source": [
    "print(\"Model and training\")\n",
    "class Word2Vec(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim):\n",
    "    super(Word2Vec, self).__init__()\n",
    "    self.target_embedding = layers.Embedding(vocab_size,\n",
    "                                      embedding_dim,\n",
    "                                      name=\"w2v_embedding\")\n",
    "    self.context_embedding = layers.Embedding(vocab_size,\n",
    "                                       embedding_dim)\n",
    "\n",
    "  def call(self, pair):\n",
    "    target, context = pair\n",
    "    # target: (batch, dummy?)  # The dummy axis doesn't exist in TF2.7+\n",
    "    # context: (batch, context)\n",
    "    if len(target.shape) == 2:\n",
    "      target = tf.squeeze(target, axis=1)\n",
    "    # target: (batch,)\n",
    "    word_emb = self.target_embedding(target)\n",
    "    # word_emb: (batch, embed)\n",
    "    context_emb = self.context_embedding(context)\n",
    "    # context_emb: (batch, context, embed)\n",
    "    dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
    "    # dots: (batch, context)\n",
    "    return dots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "322d8e9b-6851-494c-b562-6c56af4edd8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining loss function here.\n"
     ]
    }
   ],
   "source": [
    "print(\"Defining loss function here.\")\n",
    "def custom_loss(x_logit, y_true):\n",
    "      return tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=y_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e6f75234-93bd-415f-b866-3e7f895bf037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model now.\n"
     ]
    }
   ],
   "source": [
    "print(\"Building the model now.\")\n",
    "embedding_dim = 128\n",
    "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
    "word2vec.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0ffdb66e-11f1-4786-b957-0850e387ea79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Callback to log training statistics.\n"
     ]
    }
   ],
   "source": [
    "print(\"Callback to log training statistics.\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2fb0d276-570c-45e4-b968-81d2e6479760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for 20 epochs.\n",
      "Epoch 1/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.2182 - loss: 1.6089\n",
      "Epoch 2/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 62ms/step - accuracy: 0.5928 - loss: 1.5888\n",
      "Epoch 3/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.5931 - loss: 1.5298\n",
      "Epoch 4/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.5482 - loss: 1.4415\n",
      "Epoch 5/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 48ms/step - accuracy: 0.5680 - loss: 1.3446\n",
      "Epoch 6/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.6038 - loss: 1.2474\n",
      "Epoch 7/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.6419 - loss: 1.1568\n",
      "Epoch 8/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 50ms/step - accuracy: 0.6799 - loss: 1.0735\n",
      "Epoch 9/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - accuracy: 0.7107 - loss: 0.9969\n",
      "Epoch 10/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.7400 - loss: 0.9263\n",
      "Epoch 11/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 54ms/step - accuracy: 0.7672 - loss: 0.8611\n",
      "Epoch 12/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 57ms/step - accuracy: 0.7894 - loss: 0.8009\n",
      "Epoch 13/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 58ms/step - accuracy: 0.8077 - loss: 0.7455\n",
      "Epoch 14/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.8259 - loss: 0.6946\n",
      "Epoch 15/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8409 - loss: 0.6479\n",
      "Epoch 16/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step - accuracy: 0.8553 - loss: 0.6051\n",
      "Epoch 17/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 56ms/step - accuracy: 0.8660 - loss: 0.5660\n",
      "Epoch 18/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.8770 - loss: 0.5303\n",
      "Epoch 19/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.8868 - loss: 0.4977\n",
      "Epoch 20/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - accuracy: 0.8953 - loss: 0.4679\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x14ea65ecb20>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Training model for 20 epochs.\")\n",
    "word2vec.fit(dataset, epochs=20, callbacks=[tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "775b6359-3c9c-484d-b48c-23da72e6a08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing accuracy and loss now.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 17060), started 0:28:20 ago. (Use '!kill 17060' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-aad98ca0400abcea\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-aad98ca0400abcea\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"showing accuracy and loss now.\")\n",
    "#docs_infra: no_execute\n",
    "%tensorboard --logdir logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "62139e48-7dab-451e-af5d-0adecf1842bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining weights from the model.\n"
     ]
    }
   ],
   "source": [
    "print(\"Obtaining weights from the model.\")\n",
    "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cbabe7dd-eb64-483f-9176-2c796cc036f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create and save vectors and metafiles.\n",
      "Files saved successfully.\n"
     ]
    }
   ],
   "source": [
    "print(\"Create and save vectors and metafiles.\")\n",
    "out_v = io.open('C:/Users/addaw/OneDrive/Desktop/vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('C:/Users/addaw/OneDrive/Desktop/metadata.tsv', 'w', encoding='utf-8')\n",
    "print(\"Files saved successfully.\")\n",
    "for index, word in enumerate(vocab):\n",
    "  if index == 0:\n",
    "    continue  # skip 0, it's padding.\n",
    "  vec = weights[index]\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "  out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c932fc66-99f1-4cbb-a93a-25b60d4ae596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files are located at:\n",
      "Vectors file: C:/Users/addaw/OneDrive/Desktop/vectors.tsv\n",
      "Metadata file: C:/Users/addaw/OneDrive/Desktop/metadata.tsv\n"
     ]
    }
   ],
   "source": [
    "# File paths\n",
    "vectors_file_path = 'C:/Users/addaw/OneDrive/Desktop/vectors.tsv'\n",
    "metadata_file_path = 'C:/Users/addaw/OneDrive/Desktop/metadata.tsv'\n",
    "\n",
    "# Check if the files exist at the specified location\n",
    "import os\n",
    "if os.path.exists(vectors_file_path) and os.path.exists(metadata_file_path):\n",
    "    print(\"Files are located at:\")\n",
    "    print(\"Vectors file:\", vectors_file_path)\n",
    "    print(\"Metadata file:\", metadata_file_path)\n",
    "else:\n",
    "    print(\"Files not found at the specified location.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0e64d3aa-719a-4755-bfeb-0e34d32f24d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.19337258 -0.20318556 -0.17330678 -0.08985364  0.0095684  -0.21611957\n",
      " -0.06115547 -0.04213292 -0.07683181 -0.01346147  0.07778812 -0.02473289\n",
      " -0.22403194 -0.18107477  0.07811909 -0.24536036 -0.01422484 -0.24012847\n",
      " -0.08465812  0.15595421 -0.03600831  0.05431313  0.02325363  0.03042642\n",
      " -0.02321665 -0.05719719  0.10277817 -0.04997594  0.27161667  0.15970348\n",
      "  0.1652903   0.10762627 -0.05198477 -0.10737839  0.20506579  0.1559882\n",
      " -0.02573442  0.2953854  -0.01240354 -0.32764032  0.09988648 -0.29425713\n",
      "  0.124126    0.14545915 -0.03373169 -0.12321917  0.01384258 -0.004945\n",
      "  0.06247162  0.3249174  -0.1318043   0.02927196 -0.37511235 -0.10715657\n",
      " -0.10011723  0.02760358  0.02510136 -0.2877221   0.0349504  -0.0813323\n",
      "  0.38863638 -0.00752864  0.03241861  0.28781715 -0.02518696 -0.10198931\n",
      "  0.17897213  0.14308754  0.27082503 -0.11057413  0.429969    0.07336318\n",
      "  0.10728776  0.20441571 -0.01151988 -0.01857583  0.04492797  0.29041645\n",
      " -0.0137245   0.04686267  0.17430477  0.25495163  0.08172086 -0.2732519\n",
      " -0.08474501 -0.1940351   0.3277122   0.00422164  0.05954641 -0.03627817\n",
      " -0.00574033 -0.02087463 -0.0117794  -0.15315057  0.14736563 -0.02497666\n",
      "  0.14735204 -0.0871813  -0.22983569  0.18159701  0.21071813  0.30482712\n",
      "  0.15868872  0.08208795 -0.18870607  0.27717692 -0.09318156 -0.01852652\n",
      " -0.37130862 -0.04858875 -0.2326695   0.11415579  0.04456345  0.07161646\n",
      "  0.14108226 -0.06490833 -0.0550613  -0.17821866 -0.0682425  -0.08281184\n",
      "  0.20607506 -0.11280896  0.02845865  0.3286034  -0.1323088  -0.14234708\n",
      " -0.10300571  0.33522543]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load the word embeddings and vocabulary from the files\n",
    "#vectors_file_path = 'vectors.tsv'\n",
    "#metadata_file_path = 'metadata.tsv'\n",
    "vectors_file_path = 'C:/Users/addaw/OneDrive/Desktop/vectors.tsv'\n",
    "metadata_file_path = 'C:/Users/addaw/OneDrive/Desktop/metadata.tsv'\n",
    "\n",
    "# Read the vectors (embeddings) from the vectors.tsv file\n",
    "word_vectors = []\n",
    "with open(vectors_file_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        word_vectors.append([float(x) for x in line.strip().split('\\t')])\n",
    "\n",
    "# Read the vocabulary (words) from the metadata.tsv file\n",
    "vocab = []\n",
    "with open(metadata_file_path, 'r', encoding='utf-8') as f:\n",
    "    vocab = [line.strip() for line in f]\n",
    "\n",
    "# Convert the list of word vectors into a numpy array\n",
    "word_vectors = np.array(word_vectors)\n",
    "\n",
    "# Create an empty Word2Vec model \n",
    "model = Word2Vec(vector_size=word_vectors.shape[1], window=5, min_count=1)\n",
    "\n",
    "# Assign vocabulary and vectors manually\n",
    "model.wv.index_to_key = vocab  # List of words in the vocabulary\n",
    "model.wv.key_to_index = {word: idx for idx, word in enumerate(vocab)}  # Mapping from word to index\n",
    "model.wv.vectors = word_vectors  # Assign the pre-trained word vectors\n",
    "\n",
    "\n",
    "print(model.wv['love'])  # Example: print the vector for the word \"love\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "88071bd1-9702-4008-bc24-b03ca2432d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we have a small set of reviews.\n"
     ]
    }
   ],
   "source": [
    "# For illustrative purposes, a very simple movie review set. Very, very basic. \n",
    "print(\"Now we have a small set of reviews.\")\n",
    "reviews = [\"I loved this movie!\", \"It was a terrible movie.\", \"An amazing experience!\", \"This movie sucked.\", \"What an awful movie.\", \"Waste of time.\", \"The movie was good.\"]\n",
    "labels = [1, 0, 1, 0, 0, 0, 1]  # 1 = positive, 0 = negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eddd562b-3600-42bb-98bf-20d764020281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert reviews into word embeddings.\n",
      "(128,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Convert reviews into word embeddings.\")\n",
    "\n",
    "# Function to get the vector representation of a review\n",
    "def get_review_vector(review, model):\n",
    "    words = review.split()  # Tokenize the review (simple split by space)\n",
    "    word_vectors = []\n",
    "\n",
    "    # Iterate over the words in the review\n",
    "    for word in words:\n",
    "        # If the word is in the model's vocabulary, get its vector\n",
    "        if word in model.wv:\n",
    "            word_vectors.append(model.wv[word])\n",
    "    \n",
    "    # If no words were found in the vocabulary, return a zero vector\n",
    "    if len(word_vectors) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "    # Return the average of all word vectors in the review\n",
    "    review_vector = np.mean(word_vectors, axis=0)\n",
    "    return review_vector\n",
    "\n",
    "# Convert all reviews into vectors\n",
    "review_vectors = [get_review_vector(review, model) for review in reviews]\n",
    "\n",
    "# Check the shape of one of the review vectors (should match the vector size)\n",
    "print(review_vectors[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "28fa37b8-5a59-4e25-95ce-10bfeab92919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Convert the list of review vectors into a NumPy array\n",
    "X = np.array(review_vectors)  # Features (review embeddings)\n",
    "y = np.array(labels)  # Labels (positive/negative)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the Logistic Regression classifier\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Classification Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2cdce7b5-8dcf-4ac6-a4f3-229f1ccbed41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       1.0\n",
      "           1       0.00      0.00      0.00       1.0\n",
      "\n",
      "    accuracy                           0.00       2.0\n",
      "   macro avg       0.00      0.00      0.00       2.0\n",
      "weighted avg       0.00      0.00      0.00       2.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Print a detailed classification report\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fc59d4b0-7e0a-4d0a-9d17-4b6efec95e65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nResults:\\n\\nZero accuracy, but we had a very small movie review dataset.\\nMaybe a modern text will do better.\\n\\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Results:\n",
    "\n",
    "Zero accuracy, but we had a very small movie review dataset.\n",
    "Maybe a modern text will do better.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364ecb42-7d5c-4058-b9d0-1e0ca047c00a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
